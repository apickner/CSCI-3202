{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3202: Intro to AI - Fall 2020 Practicum 2\n",
    "\n",
    "## Your name:\n",
    "\n",
    "#### Collaborator's name (optional):\n",
    "\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1: Search](#p1) | [Problem 2: MDP](#p2) | [Problem 3: Q_Learn](#bot)\n",
    "\n",
    "---\n",
    "\n",
    "This practicum is due on Canvas by **10:00 PM on Saturday December 12**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  \n",
    "\n",
    "**Here are the rules:** \n",
    "\n",
    "1. All work, code and analysis, must be your own. \n",
    "2. You may use your course notes, posted lecture slides, textbooks, in-class notebooks, and homework solutions as resources.  You may also search online for answers to general knowledge questions like the form of a probability distributions or how to perform a particular operation in Python/Pandas. \n",
    "3. This is meant to be like a coding portion of your final exam. So, the instructional team will be much less helpful than we typically are with homework. For example, we will not check answers, help debug your code, and so on.\n",
    "4. If something is left open-ended, it is because we want to see how you approach the kinds of problems you will encounter in the wild, where it will not always be clear what sort of tests/methods should be applied. Feel free to ask clarifying questions though.\n",
    "5. You may **NOT** post to message boards or other online resources asking for help.  If you have a question for us, post it as a **PRIVATE** message on Piazza.  If we decide that the question is appropriate for the entire class, then we will add it to a Practicum clarifications thread. \n",
    "6. You may re-use your code or code given from in-class solutions (for e.g. Astar, MDP), but you **must cite** in comments any regions of code that were not created anew for this practicum.\n",
    "7. You may collaborate with **exactly one** of your classmates.  You must each submit your own assignments and write your own code, and may only collaborate on ideas, psuedocode, etc.  If you choose to collaborate with another student in the class, list their name under yours above.\n",
    "8. In short, **your work must be your own**. It really is that simple.\n",
    "\n",
    "Violation of the above rules will result in an immediate academic sanction (*at the very least*, you will receive a 0 on this practicum or an F in the course, depending on severity), and a trip to the Honor Code Council.\n",
    "\n",
    "**By submitting this assignment, you agree to abide by the rules given above.**\n",
    "\n",
    "***\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- You may not use late days on the practicums nor can you drop your practicum grades.\n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. \n",
    "- This should go without saying, but... For any question that asks you to calculate something, you **must show all work to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# added packages\n",
    "import heapq\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider the map of the area to the west of the Engineering Center given below, with a fairly coarse Cartesian grid superimposed.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/engineering_center_grid_zoom.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "The green square at $(x,y)=(1,15)$ is the starting location, and you would like to walk from there to the yellow square at $(25,9)$. The filled-in blue squares are obstacles, and you cannot walk through those locations.  You also cannot walk outside of this grid.\n",
    "\n",
    "Legal moves in the North/South/East/West directions have a step cost of 1. Moves in the diagonal direction (for example, from $(1,15)$ to $(2,14)$) are allowed, but they have a step cost of $\\sqrt{2}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some tuples defining the wall and state locations, for your convenience\n",
    "\n",
    "walls = [(1,y) for y in range(2,15)] + [(2,y) for y in range(3,14)] + [(3,y) for y in range(4,13)] + \\\n",
    "        [(4,y) for y in range(5,12)] + [(x,1) for x in range(5,24)] + [(10,y) for y in range(9,13)] + \\\n",
    "        [(x,y) for x in range(11,14) for y in range(9,15)] + [(14,y) for y in range(11,15)] + \\\n",
    "        [(x,y) for x in range(21,26) for y in range(11,17)] + \\\n",
    "        [(x,y) for x in [0,26] for y in range(0,18)] + [(x,y) for x in range(0,26) for y in [0,17]]\n",
    "        \n",
    "states = [(x,y) for x in range(1,26) for y in range(1,17)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "<a id='p1'></a>\n",
    "\n",
    "\n",
    "---\n",
    "## [40 pts] Part 1:  Route-finding\n",
    "In this problem, our goal is to find the path from the green to yellow squares with the **shortest total path length**.\n",
    "\n",
    "Of course, you can probably do this problem (and likely have to some degree, in your head) without a search algorithm. But that will hopefully provide a useful \"sanity check\" for your answer.\n",
    "\n",
    "#### Part A\n",
    "Write a function `adjacent_states(state)`:\n",
    "* takes a single argument `state`, which is a tuple representing a valid state in this state space\n",
    "* returns in some form the states reachable from `state` and the step costs. How exactly you do this is up to you.\n",
    "\n",
    "Print to the screen the output for `adjacent_states((1,15))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your adjacency here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "Three candidate heuristic functions might be:\n",
    "1. `heuristic_cols(state, goal)` = number of columns between the argument `state` and the `goal`\n",
    "1. `heuristic_rows(state, goal)` = number of rows between the argument `state` and the `goal`\n",
    "1. `heuristic_eucl(state, goal)` = Euclidean distance between the argument `state` and the `goal`\n",
    "\n",
    "Write a function `heuristic_max(state, goal)` that returns the maximum of all three of these heuristic functions for a given `state` and `goal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def heuristic_cols(state, goal):\n",
    "#    \n",
    "def heuristic_rows(state, goal):\n",
    "#    \n",
    "def heuristic_eucl(state, goal):\n",
    "#\n",
    "def heuristic_max(state, goal):\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "Is the Manhattan distance an admissible heuristic function for this problem?  Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "Use A\\* search and the `heuristic_max` heuristic to find the shortest path from the initial state at $(1,15)$ to the goal state at $(25,9)$. Your search **should not** build up the entire state space graph in memory. Instead, use the `adjacent_states` function from Part A, similarly to the 8-tile problem from Homework 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Astar soln:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "Make a figure depicting the optimal route from the initial state to the goal, similarly to how you depicted the maze solution in Homework 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "<a id='p2'></a>\n",
    "\n",
    "\n",
    "---\n",
    "## [40 pts] Part 2:  States and Values\n",
    "\n",
    "Winter has come, and now the area west of Engineering is icy and slippery.  As a result, there's a risk that we don't end up in the tile that we intend to move to!  In particular, if we have $k$ available actions in state $n$, the probably that we move to the state $s'$ we intend to is 75\\%, and the remaining 0.25 probability is spread equally likely across all of the other adjacent (N/S/E/W/NW/NE/SE/SW) non-wall states.\n",
    "\n",
    "In this problem, our goal is to create a policy for an agent walking in the given space west of Engineering.  Again, the goal of the agent is to navigate from start to finish, but now we want a policy for each and every location on the map.\n",
    "\n",
    "This time, however, we're going to add the same type of randomness that our process in homework 4 had.\n",
    "\n",
    "Because the state space is fully observable, we should be able to implement this as a Markov Decision process.\n",
    "\n",
    "\n",
    "#### Part A:\n",
    "\n",
    "Write the necessary functions to create *either* a **value iteration** or **policy iteration** scheme to solve for the MDP.  If you wish to follow the schema for homework 4, you may want to create an `MDP` class, with methods:\n",
    "\n",
    "- `actions`, given by the valid successor states $s'$ from all actions $a$ in state $s$ in your adjacent_states from part 1.\n",
    "- `rewards`, given by a significant positive reward for the goal state (e.g. 10) and a small negative reward for spending a long time in the system (e.g. non-wall reward of -0.01).\n",
    "- `result`, which returns the successor state $s'$ of an *actual* movement $a$ from state $s$.\n",
    "- `transition`, which returns the probability of an actual successor $s'$ given action $a$ from state $s$ using the 75\\%-25% split above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Part B:\n",
    "\n",
    "Using the MDP in part 2A, implement value iteration **or** policy iteration to calculate the utilities for each state. Also implement a function that takes as arguments an MDP object and a dictionary of state-utility pairs (key-value) and returns a dictionary for the optimal policy. The optimal policy dictionary should have state tuples as keys and the optimal move (None or any of the 8 directions) as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Part C:\n",
    "\n",
    "Sanity check your answers in part B by listing which state has the *lowest* estimated utility (should be far from the goal!) and which states have the 3 *highest* estimated utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Part D:\n",
    "\n",
    "As in part 1E, make a figure.  This time, depict the optimal policy at each location.  Your choice of visualization is up to you, but I would *recommend* taking a plot similar to the maze plot in Part 1E above (or HW 2) and plotting some choice of arrows/symbols/colors corresponding to the appropriate action in each square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "<a id='p3'></a>\n",
    "\n",
    "\n",
    "---\n",
    "## [20 pts] Part 3:  The great unknown\n",
    "\n",
    "It's nearly winter solstice, and the area west of Engineering is still icy and slippery.  \n",
    "\n",
    "As a result, there's still a risk that we don't end up in the tile that we intend to move to!  Unfortunately, for this problem we *don't know* what that risk is, and it isn't the same everywhere.  As in Problem 2, our goal is to create a policy for an agent walking in the given space west of Engineering.  Again, the goal of the agent is to navigate from start to finish, but now we want a policy for each and every location on the map\n",
    "\n",
    "Suppose there exists some function $f$ that measures the *footing* of state $s$.  Then if we choose to take the action \"move towards state $s'$\" from state $n$, the probability that we *actually arrive* in the state $s'$ we intend to is $f(s)$.  As before, the remaining $1-f(s)$ probability is spread equally likely across all of the other adjacent (N/S/E/W/NW/NE/SE/SW) non-wall states to $s$.\n",
    "\n",
    "Because our agent can't fully observe the transitions, it's going to have to pick actions and estimate their utilities from learning.  Let's use Q-learning!\n",
    "\n",
    "#### Part A:\n",
    "\n",
    "Run the following code to provide a function for and a map of the footing function $f$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOU MAY IGNORE THIS CELL, BUT MUST RUN IT TO GENERATE F\n",
    "random.seed(30)\n",
    "x = np.linspace(0,25,26)\n",
    "y = np.linspace(0,25,26)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "f1 = np.zeros(X.shape)\n",
    "f2 = np.zeros(X.shape)\n",
    "f3 = np.zeros(X.shape)\n",
    "f4 = np.zeros(X.shape)\n",
    "\n",
    "mu1, mu2, mu3, mu4=[17,12],[17,11],[11,8],[11,6]\n",
    "covar1, covar2, covar3, covar4= [[16,8],[8,16]],[[12,.5],[.5,12]],[[4,.8],[.8,4]],[[.8,12],[.8,12]]\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        f1[i,j] = 6*stats.multivariate_normal.pdf(x=(X[i,j],Y[i,j]), mean=mu1, cov=covar1)\n",
    "        f2[i,j] = 3*stats.multivariate_normal.pdf(x=(X[i,j],Y[i,j]), mean=mu2, cov=covar2)\n",
    "        f3[i,j] = stats.multivariate_normal.pdf(x=(X[i,j],Y[i,j]), mean=mu3, cov=covar3)\n",
    "        f4[i,j] = 1*stats.multivariate_normal.pdf(x=(X[i,j],Y[i,j]), mean=mu4, cov=covar4)\n",
    "        \n",
    "f =f1+f2+f3+f4    \n",
    "f=1-(f/np.max(f))**(1/3)\n",
    "\n",
    "#PLOTTING:\n",
    "fig, ax = plt.subplots(1,1, figsize=(7,5))\n",
    "my_levels = np.linspace(0, 1, 11)\n",
    "labels = [str(lv) for lv in my_levels]\n",
    "cp = ax.contour(X, Y, f, levels=my_levels)\n",
    "plt.clabel(cp, inline=1, fontsize=10)\n",
    "ax.set(xlim=(0, 25), ylim=(0, 16))\n",
    "plt.title('Footing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can access f directly using indices if you use the TRANPOSE of the coordinates of each point\n",
    "#which this footing function does\n",
    "def footing(x,y):\n",
    "    return f[y,x]\n",
    "\n",
    "print(\"It's icy at (12,8), with almost no footing:\", footing(12,8))\n",
    "print(\"It's better at (8,12):\", footing(8,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B:\n",
    "\n",
    "We're going to implement Q-learning instead of an MDP.\n",
    "\n",
    "You will probably want to create a dictionary of the form discussed at the end of the in-class notebook for Q-learning, where each valid tuple is the first key and each valid move from that location is the second key.  You then should have the estimated utilities of each action saved in the resulting dictionary.  You may include other information if desired, but nothing else should be absolutely necessary.\n",
    "\n",
    "After initialization, print the elements of the dictionary corresponding to the (4,4) location.  Note that there should be 7 subdictionaries for the 6 possible neighbors and the `None` action, and within each action the initial Q-value should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Q, initialize all the Q-utilities as 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C:\n",
    "\n",
    "Use the given `footing` function to modify your `transitions` from the MDP so the probabilities of result given action now flow from the icy model above.\n",
    "\n",
    "Perform at least 1000 training epochs, where each starts at a *random* location from the valid states (this can help if find the goal state faster!).\n",
    "\n",
    "For each epoch, take *at most* 100 actions, or until the goal is reached.  You may choose these actions by any schema you desire, but I recommend the $\\varepsilon$-greedy agent that chooses the \"best available\" action 80% of the time and explores the other 20%. \n",
    "\n",
    "Show graphs depicting the actual paths taken for the last 2 of the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D:\n",
    "\n",
    "As in part 2D, make a figure depicting the optimal policy at each location.  Does your agent actually try to avoid the ice, compared to how it behaved in the MDP in Part 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
